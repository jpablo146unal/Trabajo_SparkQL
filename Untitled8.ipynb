{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Evaluacion iSofware, master=local[*]) created by __init__ at <ipython-input-1-81be72d1ccca>:16 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-81be72d1ccca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# definimos un espacio o contexto para la App\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mContextoSpark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAppSpark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# inicio una sesión en el espacio de la App\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Evaluacion iSofware, master=local[*]) created by __init__ at <ipython-input-1-81be72d1ccca>:16 "
     ]
    }
   ],
   "source": [
    "# Agrege acá el código para importar las librerias\n",
    "\n",
    "# La libreria para \"encontrar el sevicio\" de Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Librerias para \"gestionar el servicio\" de Spark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "# Creamos una aplicación Spark en el Servicio\n",
    "# Tenga cuidado con las tildes o caracteres especiales en el nombre de la app\n",
    "AppSpark = SparkConf().setAppName(\"Evaluacion iSofware\")\n",
    "\n",
    "# definimos un espacio o contexto para la App\n",
    "ContextoSpark=SparkContext(conf=AppSpark)\n",
    "\n",
    "# inicio una sesión en el espacio de la App\n",
    "SesionSpark = SparkSession(ContextoSpark)\n",
    "\n",
    "# inicio del espacio o contexto SQL\n",
    "ContextoSql = SQLContext(sparkContext=ContextoSpark, sparkSession=SesionSpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-05 01:19:54--  https://raw.githubusercontent.com/jpablo146unal/Trabajo_SparkQL/main/Asistencia_del_Pitch.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 93121 (91K) [text/plain]\n",
      "Saving to: 'Asistencia_del_Pitch.csv.3'\n",
      "\n",
      "Asistencia_del_Pitc 100%[===================>]  90.94K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2021-05-05 01:19:55 (2.48 MB/s) - 'Asistencia_del_Pitch.csv.3' saved [93121/93121]\n",
      "\n",
      "--2021-05-05 01:19:55--  https://raw.githubusercontent.com/jpablo146unal/Trabajo_SparkQL/main/Evaluacion_del_Pitch.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 111944 (109K) [text/plain]\n",
      "Saving to: 'Evaluacion_del_Pitch.csv.3'\n",
      "\n",
      "Evaluacion_del_Pitc 100%[===================>] 109.32K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-05-05 01:19:56 (3.28 MB/s) - 'Evaluacion_del_Pitch.csv.3' saved [111944/111944]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jpablo146unal/Trabajo_SparkQL/main/Asistencia_del_Pitch.csv\n",
    "!wget https://raw.githubusercontent.com/jpablo146unal/Trabajo_SparkQL/main/Evaluacion_del_Pitch.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/tmp/Asistencia_del_Pitch.csv': File exists\n",
      "copyFromLocal: `/tmp/Evaluacion_del_Pitch.csv': File exists\n",
      "-rw-r--r--   1 root supergroup      93121 2021-05-04 01:51 /tmp/Asistencia_del_Pitch.csv\n",
      "-rw-r--r--   1 root supergroup     111944 2021-05-04 01:51 /tmp/Evaluacion_del_Pitch.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargue los datos en la carpeta datalake y luego del /datalake al HDFS (Hadoop File System)\n",
    "# Recuerda usar ! para ejecutar el comando en el shell.\n",
    "!hdfs dfs -copyFromLocal Asistencia_del_Pitch.csv /tmp/\n",
    "!hdfs dfs -copyFromLocal Evaluacion_del_Pitch.csv /tmp/\n",
    "!hdfs dfs -ls /tmp/*csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 2. Cree dos tablas SparkSQL y almacene el csv en su correspondiente tabla.\n",
    "asistencia =SesionSpark.read.load('/tmp/Asistencia_del_Pitch.csv',format=\"csv\",sep=',',inferSchema='true',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 2. Cree dos tablas SparkSQL y almacene el csv en su correspondiente tabla.\n",
    "evaluacion =SesionSpark.read.load('/tmp/Evaluacion_del_Pitch.csv',format=\"csv\",sep=',',inferSchema='true',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+------------------------+\n",
      "|      Marca temporal|   Nombre de usuario|Equipo al que perteneces:|Equipo que va a exponer:|\n",
      "+--------------------+--------------------+-------------------------+------------------------+\n",
      "|2021/04/08 8:07:5...|jppinedal@unal.ed...|     Pertenezco a: Ser...|    Expone: Servicios...|\n",
      "|2021/04/08 8:09:3...|dgarciabl@unal.ed...|     Pertenezco a: Lo ...|    Expone: Lo tengo ...|\n",
      "|2021/04/08 8:09:5...|auarbelaeza@unal....|     Pertenezco a: Ges...|    Expone: Te lo cam...|\n",
      "|2021/04/08 8:09:5...|jmunozhe@unal.edu.co|     Pertenezco a: Lo ...|    Expone: Te lo cam...|\n",
      "|2021/04/08 8:10:0...|dgerenal@unal.edu.co|     Pertenezco a: Te ...|    Expone: Te lo cam...|\n",
      "+--------------------+--------------------+-------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # 2. Cree dos tablas SparkSQL y almacene el csv en su correspondiente tabla.\n",
    "asistencia.select(['Marca temporal', 'Nombre de usuario', 'Equipo al que perteneces:', 'Equipo que va a exponer:']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reemplazando espacios por underline\n",
    "new_column_name_list = list(map(lambda x: x.replace(\" \", \"_\"), asistencia.columns))\n",
    "asistencia = asistencia.toDF(*new_column_name_list)\n",
    "#reemplazando : por nada\n",
    "new_column_name_list = list(map(lambda x: x.replace(\":\", \"\"), asistencia.columns))\n",
    "asistencia = asistencia.toDF(*new_column_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Marca_temporal: string (nullable = true)\n",
      " |-- Nombre_de_usuario: string (nullable = true)\n",
      " |-- Equipo_al_que_perteneces: string (nullable = true)\n",
      " |-- Equipo_que_va_a_exponer: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asistencia.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reemplazando espacios por underline\n",
    "new_column_name_list = list(map(lambda x: x.replace(\" \", \"_\"), evaluacion.columns))\n",
    "evaluacion = evaluacion.toDF(*new_column_name_list)\n",
    "#reemplazando : por nada\n",
    "new_column_name_list = list(map(lambda x: x.replace(\":\", \"\"), evaluacion.columns))\n",
    "evaluacion = evaluacion.toDF(*new_column_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Marca_temporal: string (nullable = true)\n",
      " |-- Nombre_de_usuario: string (nullable = true)\n",
      " |-- Equipo_que_vas_a_evaluar: string (nullable = true)\n",
      " |-- Introducción_El_equipo_responde_adecuadamente_¿Quiénes_son_y_por_qué_están_aquí?: integer (nullable = true)\n",
      " |-- Equipo_El_equipo_responde_adecuadamente_¿Quiénes_están_detrás_de_la_idea_y_cuál_es_su_función?: integer (nullable = true)\n",
      " |-- Problema_El_equipo_responde_adecuadamente_¿Qué_problema_resolverá?,_¿es_realmente_un_problema?: integer (nullable = true)\n",
      " |-- Ventajas_El_equipo_responde_adecuadamente_¿Por_qué_su_solución_es_especial?,_¿qué_la_hace_distinta_de_otras?: integer (nullable = true)\n",
      " |-- Solución_El_equipo_responde_adecuadamente_¿Cómo_piensa_resolver_el_problema?: integer (nullable = true)\n",
      " |-- Producto_El_equipo_responde_adecuadamente_¿Cómo_funciona_el_producto_o_servicio?_Muestra_algunos_ejemplos.: integer (nullable = true)\n",
      " |-- Tracción_El_equipo_responde_adecuadamente_si_cuenta_con_clientes_que_demuestran_potencial.: integer (nullable = true)\n",
      " |-- Mercado_El_equipo_responde_conoce,_o_por_lo_menos_intentar_predecir,_el_tamaño_del_mercado_que_impactará.: integer (nullable = true)\n",
      " |-- Competencia_El_equipo_responde_adecuadamente_¿Cuáles_son_las_soluciones_alternativas_al_problema_que_plantea?: integer (nullable = true)\n",
      " |-- Modelo_de_negocio_El_equipo_responde_adecuadamente_¿Cómo_hará_dinero?_: integer (nullable = true)\n",
      " |-- Inversión_El_equipo_responde_adecuadamente_¿Cuál_es_su_presupuesto_y_cuánto_espera_ganar?: integer (nullable = true)\n",
      " |-- Contacto_El_equipo_deja_los_datos_al_cliente_y_muestra_cómo_pueden_contactarle.: integer (nullable = true)\n",
      " |-- Exposición_¿Qué_tan_coordinados_estaban_los_expositores?: integer (nullable = true)\n",
      " |-- Exposición_¿Los_expositores_se_expresaron_con_claridad_y_se_hicieron_entender?: integer (nullable = true)\n",
      " |-- Exposición_Las_diapositivas_son_claras_y_coherentes_y_apoyaron_adecuadamente_la_exposición.: integer (nullable = true)\n",
      " |-- Suponiendo_que_eres_inversionista,_¿Estarías_dispuesto_a_invertir_dinero_en_este_equipo?_(esta_pregunta_no_se_pondera_en_la_nota): string (nullable = true)\n",
      " |-- Observaciones_para_el_equipo,_estas_observaciones_las_debe_considerar_el_equipo_para_mejorar_la_siguiente_presentación.: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluacion.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `Asistencia_del_Pitch.csv': File exists\n",
      "put: `Evaluacion_del_Pitch.csv': File exists\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup      93121 2021-05-04 02:50 Asistencia_del_Pitch.csv\n",
      "-rw-r--r--   1 root supergroup     111944 2021-05-04 02:50 Evaluacion_del_Pitch.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -put Asistencia_del_Pitch.csv\n",
    "!hadoop fs -put Evaluacion_del_Pitch.csv\n",
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table asistencia already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21.sql.\n: org.apache.spark.sql.AnalysisException: Table asistencia already exists.;\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4e604e471a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m'Asistencia_del_Pitch.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     header 'true')\n\u001b[0;32m---> 11\u001b[0;31m \"\"\");\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table asistencia already exists.;'"
     ]
    }
   ],
   "source": [
    "###2. Creación tabla asistencia\n",
    "###\n",
    "SesionSpark.sql(\"\"\"\n",
    "CREATE TABLE \n",
    "    asistencia \n",
    "USING com.databricks.spark.csv \n",
    "OPTIONS (\n",
    "    sep=',',\n",
    "    path 'Asistencia_del_Pitch.csv', \n",
    "    header 'true')\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table evaluacion already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21.sql.\n: org.apache.spark.sql.AnalysisException: Table evaluacion already exists.;\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d4198c26c8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m'Evaluacion_del_Pitch.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     header 'true')\n\u001b[0;32m---> 10\u001b[0;31m \"\"\");\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table evaluacion already exists.;'"
     ]
    }
   ],
   "source": [
    "###2. Creación tabla evaluacion\n",
    "SesionSpark.sql(\"\"\"\n",
    "CREATE TABLE \n",
    "    evaluacion \n",
    "USING com.databricks.spark.csv \n",
    "OPTIONS (\n",
    "    sep=',',\n",
    "    path 'Evaluacion_del_Pitch.csv', \n",
    "    header 'true')\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "| default| asistencia|      false|\n",
      "| default| evaluacion|      false|\n",
      "|        |asistencia1|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Confirmación creación de tablas\n",
    "SesionSpark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   Nombre_de_Usuario|\n",
      "+--------------------+\n",
      "| abanolc@unal.edu.co|\n",
      "|acastrillonv@unal...|\n",
      "| aflemag@unal.edu.co|\n",
      "|anapariciom@unal....|\n",
      "|angutierrezb@unal...|\n",
      "|anoriega@unal.edu.co|\n",
      "|auarbelaeza@unal....|\n",
      "| bocampo@unal.edu.co|\n",
      "|cgiraldo@unal.edu.co|\n",
      "|cjfunezg@unal.edu.co|\n",
      "|cquinchiar@unal.e...|\n",
      "| dadazam@unal.edu.co|\n",
      "|daestradam@unal.e...|\n",
      "|davgarciava@unal....|\n",
      "|dballesteroso@una...|\n",
      "|  dbrito@unal.edu.co|\n",
      "|dcadavid@unal.edu.co|\n",
      "|dcardonaal@unal.e...|\n",
      "|dchavarriar@unal....|\n",
      "|dgarciabl@unal.ed...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Consulte el listado total de estudiantes (correos electrónicos) del \n",
    "# curso de Ingeniería de Software, ordenados alfabéticamente\n",
    "asistencia.createOrReplaceTempView(\"asistencia1\")\n",
    "SesionSpark.sql(\"select distinct(Nombre_de_Usuario) from asistencia1 order by(Nombre_de_Usuario) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------------------+--------------------+\n",
      "|   Nombre_de_Usuario|count(Marca_Temporal)| min(Marca_Temporal)| max(Marca_Temporal)|\n",
      "+--------------------+---------------------+--------------------+--------------------+\n",
      "|samolinap@unal.ed...|                   10|2021/04/08 8:11:4...|2021/04/13 9:37:2...|\n",
      "| jruedat@unal.edu.co|                    8|2021/04/08 8:12:5...|2021/04/13 9:41:2...|\n",
      "|nvalenciat@unal.e...|                    8|2021/04/08 8:10:1...|2021/04/13 9:39:3...|\n",
      "|serendona@unal.ed...|                    8|2021/04/08 8:10:4...|2021/04/13 9:40:2...|\n",
      "|jgutierrezlo@unal...|                    8|2021/04/08 8:12:0...|2021/04/13 9:39:0...|\n",
      "|dgerenal@unal.edu.co|                    8|2021/04/08 8:10:0...|2021/04/13 9:41:2...|\n",
      "|cjfunezg@unal.edu.co|                    8|2021/04/08 8:12:1...|2021/04/13 9:41:4...|\n",
      "|emflorezb@unal.ed...|                    8|2021/04/08 8:14:0...|2021/04/13 9:45:4...|\n",
      "|dchavarriar@unal....|                    8|2021/04/08 8:13:0...|2021/04/13 9:43:5...|\n",
      "|fguerrerot@unal.e...|                    8|2021/04/08 8:13:0...|2021/04/13 9:40:1...|\n",
      "|jdroldano@unal.ed...|                    1|2021/04/08 8:20:1...|2021/04/08 8:20:1...|\n",
      "|jovillarrealm@una...|                   10|2021/04/08 10:02:...|2021/04/13 9:42:5...|\n",
      "|juriveras@unal.ed...|                    8|2021/04/08 8:13:3...|2021/04/13 9:41:1...|\n",
      "|dguardia@unal.edu.co|                    8|2021/04/08 8:12:5...|2021/04/13 9:40:3...|\n",
      "| lrdiaza@unal.edu.co|                    7|2021/04/08 8:14:5...|2021/04/13 9:45:2...|\n",
      "|fmiranda@unal.edu.co|                    8|2021/04/08 8:10:1...|2021/04/13 9:45:0...|\n",
      "|dcadavid@unal.edu.co|                    8|2021/04/08 8:13:0...|2021/04/13 9:40:5...|\n",
      "| bocampo@unal.edu.co|                    8|2021/04/08 8:13:3...|2021/04/13 9:42:1...|\n",
      "|rmbuilesm@unal.ed...|                    9|2021/04/08 8:14:1...|2021/04/13 9:41:3...|\n",
      "|jmunozhe@unal.edu.co|                    8|2021/04/08 8:09:5...|2021/04/13 9:10:1...|\n",
      "+--------------------+---------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Consulte la cantidad de asistencias registradas por estudiante; además, la fecha y hora de la primera asistencia\n",
    "# y la fecha y hora de la última asistencia\n",
    "# Tu código a continuación...\n",
    "asistencia.createOrReplaceTempView(\"asistencia1\")\n",
    "SesionSpark.sql(\"select (Nombre_de_Usuario), count(Marca_Temporal), min(Marca_Temporal), max(Marca_Temporal) from asistencia1 group by(Nombre_de_Usuario) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|   Nombre_de_Usuario|count(Marca_Temporal)|\n",
      "+--------------------+---------------------+\n",
      "|jdroldano@unal.ed...|                    1|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Consulte el listado de estudiantes que asistieron a 2 presentaciones o menos (una).\n",
    "# Tu código a continuación...\n",
    "asistencia.createOrReplaceTempView(\"asistencia1\")\n",
    "SesionSpark.sql(\"select (Nombre_de_Usuario), count(Marca_Temporal) from asistencia1 group by(Nombre_de_Usuario) having count(Marca_Temporal) in (1,2) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|Nombre_de_Usuario|count(Marca_Temporal)|\n",
      "+-----------------+---------------------+\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Consulte el listado de estudiantes que no asistieron a ninguna presentación.\n",
    "# Tu código a continuación...\n",
    "asistencia.createOrReplaceTempView(\"asistencia1\")\n",
    "SesionSpark.sql(\"select (Nombre_de_Usuario), count(Marca_Temporal) from asistencia1 group by(Nombre_de_Usuario) having count(Marca_Temporal) < 1 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------+\n",
      "|Equipo_al_que_perteneces|   Nombre_de_Usuario|\n",
      "+------------------------+--------------------+\n",
      "|    Pertenezco a: Ges...| abanolc@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|acastrillonv@unal...|\n",
      "|    Pertenezco a: Mi ...| aflemag@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|anapariciom@unal....|\n",
      "|    Pertenezco a: Ser...|anapariciom@unal....|\n",
      "|    Pertenezco a: Adm...|angutierrezb@unal...|\n",
      "|    Pertenezco a: Te ...|anoriega@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|auarbelaeza@unal....|\n",
      "|    Pertenezco a: Ser...| bocampo@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|cgiraldo@unal.edu.co|\n",
      "|    Pertenezco a: Ser...|cjfunezg@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|cquinchiar@unal.e...|\n",
      "|    Pertenezco a: Mi ...| dadazam@unal.edu.co|\n",
      "|    Pertenezco a: Lo ...|daestradam@unal.e...|\n",
      "|    Pertenezco a: Ges...|davgarciava@unal....|\n",
      "|    Pertenezco a: Te ...|dballesteroso@una...|\n",
      "|    Pertenezco a: Ges...|  dbrito@unal.edu.co|\n",
      "|    Pertenezco a: Mi ...|dcadavid@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|dcardonaal@unal.e...|\n",
      "|    Pertenezco a: Ges...|dchavarriar@unal....|\n",
      "|    Pertenezco a: Lo ...|dgarciabl@unal.ed...|\n",
      "|    Pertenezco a: Te ...|dgerenal@unal.edu.co|\n",
      "|    Pertenezco a: Lo ...|dgiraldolo@unal.e...|\n",
      "|    Pertenezco a: Ges...|dguardia@unal.edu.co|\n",
      "|    Pertenezco a: Ser...|dosoriom@unal.edu.co|\n",
      "|    Pertenezco a: Adm...| eapenad@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|elondonoc@unal.ed...|\n",
      "|    Pertenezco a: Adm...|emflorezb@unal.ed...|\n",
      "|    Pertenezco a: Ges...|emlopezr@unal.edu.co|\n",
      "|    Pertenezco a: Mi ...|eporrasm@unal.edu.co|\n",
      "|    Pertenezco a: Mi ...|esgarciac@unal.ed...|\n",
      "|    Pertenezco a: Ges...|favasquezj@unal.e...|\n",
      "|    Pertenezco a: Te ...|fguerrerot@unal.e...|\n",
      "|    Pertenezco a: Mi ...|fmiranda@unal.edu.co|\n",
      "|    Pertenezco a: Adm...|framirezf@unal.ed...|\n",
      "|    Pertenezco a: Ges...|hriveraa@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|jcardonaso@unal.e...|\n",
      "|    Pertenezco a: Lo ...|jcorreapa@unal.ed...|\n",
      "|    Pertenezco a: Ser...|jdroldano@unal.ed...|\n",
      "|    Pertenezco a: Lo ...|jfernandezmo@unal...|\n",
      "|    Pertenezco a: Te ...|jgutierrezlo@unal...|\n",
      "|    Pertenezco a: Adm...|jhcordoba@unal.ed...|\n",
      "|    Pertenezco a: Lo ...|jhcordoba@unal.ed...|\n",
      "|    Pertenezco a: Ges...|jjmonsalvem@unal....|\n",
      "|    Pertenezco a: Lo ...|jlopezpe@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|jmonsalveb@unal.e...|\n",
      "|    Pertenezco a: Ser...|jmunozbo@unal.edu.co|\n",
      "|    Pertenezco a: Lo ...|jmunozhe@unal.edu.co|\n",
      "|    Pertenezco a: Adm...| jortize@unal.edu.co|\n",
      "|    Pertenezco a: Mi ...|jovillarrealm@una...|\n",
      "|    Pertenezco a: Ser...|jppinedal@unal.ed...|\n",
      "|    Pertenezco a: Ser...| jruedat@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|jspatinoa@unal.ed...|\n",
      "|    Pertenezco a: Lo ...|jubuitrago@unal.e...|\n",
      "|    Pertenezco a: Ges...|jucardonaa@unal.e...|\n",
      "|    Pertenezco a: Ges...|jugutierrezt@unal...|\n",
      "|    Pertenezco a: Mi ...|juochoag@unal.edu.co|\n",
      "|    Pertenezco a: Te ...|juortizt@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|juriveras@unal.ed...|\n",
      "|    Pertenezco a: Ges...|keariasb@unal.edu.co|\n",
      "|    Pertenezco a: Ges...| kmolano@unal.edu.co|\n",
      "|    Pertenezco a: Adm...| kmolano@unal.edu.co|\n",
      "|    Pertenezco a: Lo ...|lbarcelo@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|  lmayar@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|   lmazo@unal.edu.co|\n",
      "|    Pertenezco a: Ges...| lrdiaza@unal.edu.co|\n",
      "|    Pertenezco a: Adm...|lumarinb@unal.edu.co|\n",
      "|    Pertenezco a: Mi ...| mcallea@unal.edu.co|\n",
      "|    Pertenezco a: Ges...|mcarvajalsa@unal....|\n",
      "|    Pertenezco a: Ges...|mcarvajalsa@unal....|\n",
      "|    Pertenezco a: Te ...|mhernandezbe@unal...|\n",
      "|    Pertenezco a: Te ...|mimorenov@unal.ed...|\n",
      "|    Pertenezco a: Ges...|mizapataa@unal.ed...|\n",
      "|    Pertenezco a: Adm...|nsalinas@unal.edu.co|\n",
      "|    Pertenezco a: Te ...|nvalenciat@unal.e...|\n",
      "|    Pertenezco a: Te ...|parestrepoo@unal....|\n",
      "|    Pertenezco a: Adm...|rmbuilesm@unal.ed...|\n",
      "|    Pertenezco a: Ser...|sacastrot@unal.ed...|\n",
      "|    Pertenezco a: Adm...|saespinosab@unal....|\n",
      "|    Pertenezco a: Adm...|samolinap@unal.ed...|\n",
      "|    Pertenezco a: Te ...|sasalazarr@unal.e...|\n",
      "|    Pertenezco a: Ges...|seagudeloo@unal.e...|\n",
      "|    Pertenezco a: Ser...|sebermudezg@unal....|\n",
      "|    Pertenezco a: Adm...|sednarvaezna@unal...|\n",
      "|    Pertenezco a: Ges...|serendona@unal.ed...|\n",
      "|    Pertenezco a: Mi ...|   tleon@unal.edu.co|\n",
      "|    Pertenezco a: Lo ...| wocampo@unal.edu.co|\n",
      "+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Consulte los integrantes por cada equipo al que pertenecen.\n",
    "# Tu código a continuación...\n",
    "asistencia.createOrReplaceTempView(\"asistencia1\")\n",
    "SesionSpark.sql(\"select distinct (Equipo_al_que_perteneces),(Nombre_de_Usuario) from asistencia1 order by 2\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----------------------+\n",
      "|count(DISTINCT Nombre_de_Usuario)|Equipo_que_va_a_exponer|\n",
      "+---------------------------------+-----------------------+\n",
      "|                               80|   Expone: Mi Sanand...|\n",
      "|                               78|   Expone: Gestión d...|\n",
      "|                               80|   Expone: Servicios...|\n",
      "|                               77|   Expone: Gestión d...|\n",
      "|                               80|   Expone: Administr...|\n",
      "|                               78|   Expone: Lo tengo ...|\n",
      "|                               81|   Expone: Gestión d...|\n",
      "|                               80|   Expone: Te lo cam...|\n",
      "|                                3|   Expone: Deserción...|\n",
      "+---------------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Consulte la cantidad de asistentes por presentación, sin considerar los asistentes que pertenecen \n",
    "# al equipo que realizó la presentación.\n",
    "# Tu código a continuación...\n",
    "asistencia.createOrReplaceTempView(\"asistencia1\")\n",
    "SesionSpark.sql(\"\"\"\n",
    "select count(distinct (Nombre_de_Usuario)), (Equipo_que_va_a_exponer) \n",
    "    from asistencia1 \n",
    "    where (Equipo_al_que_perteneces) != (Equipo_que_va_a_exponer)\n",
    "group by (Equipo_que_va_a_exponer) \n",
    "\"\"\").show()\n",
    "#(Equipo_que_va_a_exponer) =! (Equipo_al_que_perteneces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------+---------------------------------------------------+------------------------+\n",
      "| Nombre_de_Usuario|Equipo_que_vas_a_evaluar|replace(Equipo_al_que_perteneces, Pertenezco a: , )|Equipo_que_vas_a_evaluar|\n",
      "+------------------+------------------------+---------------------------------------------------+------------------------+\n",
      "|lmayar@unal.edu.co|    Gestión de progra...|                               Gestión de progra...|    Gestión de progra...|\n",
      "+------------------+------------------------+---------------------------------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Consutar cuáles integrantes evaluaron a su propio equipo. Estas evaluaciones no serán válidas, pues un\n",
    "# integrante no puede evaluar a su propio equipo.\n",
    "# Tu código a continuación...\n",
    "evaluacion.createOrReplaceTempView(\"evaluacion1\")\n",
    "SesionSpark.sql(\"\"\"\n",
    "select distinct e.Nombre_de_Usuario, e.Equipo_que_vas_a_evaluar, replace(a.Equipo_al_que_perteneces,'Pertenezco a: ',''),e.Equipo_que_vas_a_evaluar\n",
    "    from evaluacion1 e inner join asistencia1 a\n",
    "        on a.Nombre_de_Usuario = e.Nombre_de_Usuario\n",
    "        and replace(a.Equipo_al_que_perteneces,'Pertenezco a: ','') = e.Equipo_que_vas_a_evaluar\n",
    "where e.Nombre_de_Usuario = 'lmayar@unal.edu.co'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
